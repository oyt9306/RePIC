# ğŸ” RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models
Authors: 
Yeongtak Oh, Jisoo Mok, DoHyun Chung, Juhyeon Shin, Sangha Park, Johan Barthelemy, and Sungroh Yoon
Abstract: 
Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task. 

## ğŸ“¦ Installation Guide (Inference Only)

Our codebase has been tested on **CUDA 12.4**. Please follow the instructions below:

```bash
# Create and activate conda environment
conda create -n RePIC python=3.11 -y
conda activate RePIC

# Install CUDA 12.4 toolkit
conda install nvidia/label/cuda-12.4.0::cuda-toolkit

# Setup permissions and dependencies
chmod 755 *.sh
bash ./setup.sh

# Set up kernel
conda install ipykernel -y
python -m ipykernel install --user --name RePIC --display-name RePIC
```

> âš ï¸ If installation fails, it may be due to issues with the `flash_attention_2` library.  
Please refer to the official [Qwen2.5-VL repository](https://github.com/QwenLM/Qwen2.5-VL) for alternative inference guidance.

We have only tested inference with the **`flash_attention_2`** setup. Logs and example outputs are included in [`inference_example.ipynb`](./inference_example.ipynb).

---

## ğŸ–¼ï¸ Visualization Example

You can run the `gradio_example_4_concept.ipynb` notebooks for **visualization with pre-generated captions** without installing the environmental settings.

> ğŸ“Œ Note: We curated the database and query images for a 4-concept setting, all evaluation images are credited to [RAP-MLLM](https://arxiv.org/abs/2410.13360).

Feel free to try it out!

---

## ğŸ§ª Inference Example

The `inference_example.ipynb` notebook contains:
- Scripts to run **inference with your own queries**
- Reproducible code for **Figure 1, Figure A.1 and Figure A.2** in our paper

---

## ğŸ‹ï¸ Training & Evaluation

Training and evaluation code is currently **under construction**.  
We plan to release it as open source in the near future!

---

## ğŸ“ Data Download

You can download our **5K dataset** used for training and evaluation here:

ğŸ“ [Google Drive Link](https://drive.google.com/file/d/1DKPmLI58NZUpSUFEUgzpUmMKKPK3Oguc/view?usp=sharing)

> âœ… Note: We only used a **2K subset** of this dataset for training purposes.

---

## ğŸ™ Acknowledgements

We gratefully acknowledge the following open-source repositories and resources that supported our work:

- ğŸ”— [VLM-R1 (Om-AI-Lab)](https://github.com/om-ai-lab/VLM-R1)  
- ğŸ”— [Qwen2.5-VL (QwenLM)](https://github.com/QwenLM/Qwen2.5-VL)
